{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d952b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "\n",
    "import re #for working with regular expression\n",
    "import nltk #for natural language processing (nlp)\n",
    "import spacy #also for nlp\n",
    "import string #This is a module, Python also has built-in class str, these are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbb7d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'/Users/vishalverma/Vishal/Kaggle/NLP on Tweets/Data/train.csv')\n",
    "test_df = pd.read_csv(r'/Users/vishalverma/Vishal/Kaggle/NLP on Tweets/Data/test.csv')\n",
    "sample_sub1 = pd.read_csv(r'/Users/vishalverma/Vishal/Kaggle/NLP on Tweets/Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe345dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging train and test dataframe for performing text-preprocessing\n",
    "train_df_copy = train_df\n",
    "train_df = train_df.drop('target', axis = 1)\n",
    "frames = [train_df,test_df]\n",
    "train_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eea2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting everything in Lower case\n",
    "train_df['lowered_text'] = train_df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8fdfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    our deeds are the reason of this #earthquake m...\n",
      "1               forest fire near la ronge sask. canada\n",
      "2    all residents asked to 'shelter in place' are ...\n",
      "3    13,000 people receive #wildfires evacuation or...\n",
      "4    just got sent this photo from ruby #alaska as ...\n",
      "5    #rockyfire update => california hwy. 20 closed...\n",
      "6    #flood #disaster heavy rain causes flash flood...\n",
      "7    i'm on top of the hill and i can see a fire in...\n",
      "8    there's an emergency evacuation happening now ...\n",
      "9    i'm afraid that the tornado is coming to our a...\n",
      "Name: lowered_text, dtype: object\n",
      "0    our deeds are the reason of this earthquake ma...\n",
      "1                forest fire near la ronge sask canada\n",
      "2    all residents asked to shelter in place are be...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4    just got sent this photo from ruby alaska as s...\n",
      "5    rockyfire update  california hwy 20 closed in ...\n",
      "6    flood disaster heavy rain causes flash floodin...\n",
      "7    im on top of the hill and i can see a fire in ...\n",
      "8    theres an emergency evacuation happening now i...\n",
      "9     im afraid that the tornado is coming to our area\n",
      "Name: lowered_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation\n",
    "punctuation=string.punctuation\n",
    "mapping=str.maketrans(\"\",\"\",punctuation)\n",
    "\n",
    "def remove_punctuation(in_str):\n",
    "    return in_str.translate(mapping)\n",
    "\n",
    "print(train_df['lowered_text'].head(10))   \n",
    "train_df['lowered_text']=train_df[\"lowered_text\"].apply(lambda x: remove_punctuation(x))\n",
    "print(train_df['lowered_text'].head(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72964140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    our deeds are the reason of this earthquake ma...\n",
      "1                forest fire near la ronge sask canada\n",
      "2    all residents asked to shelter in place are be...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4    just got sent this photo from ruby alaska as s...\n",
      "5    rockyfire update  california hwy 20 closed in ...\n",
      "6    flood disaster heavy rain causes flash floodin...\n",
      "7    im on top of the hill and i can see a fire in ...\n",
      "8    theres an emergency evacuation happening now i...\n",
      "9     im afraid that the tornado is coming to our area\n",
      "Name: lowered_text, dtype: object\n",
      "0        deeds reason earthquake may allah forgive us \n",
      "1               forest fire near la ronge sask canada \n",
      "2    residents asked shelter place notified officer...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4    got sent photo ruby alaska smoke wildfires pou...\n",
      "5    rockyfire update california hwy 20 closed dire...\n",
      "6    flood disaster heavy rain causes flash floodin...\n",
      "7                          im top hill see fire woods \n",
      "8    theres emergency evacuation happening building...\n",
      "9                       im afraid tornado coming area \n",
      "Name: lowered_text_stop_removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removing Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_eng=stopwords.words('english')\n",
    "\n",
    "print(train_df[\"lowered_text\"].head(10)) #before\n",
    "\n",
    "def remove_stopwords(in_str):\n",
    "    new_str=''\n",
    "    words=in_str.split()\n",
    "    for tx in words:\n",
    "        if tx not in stopwords_eng:\n",
    "            new_str=new_str + tx + \" \"\n",
    "    return new_str\n",
    "\n",
    "train_df['lowered_text_stop_removed']=train_df[\"lowered_text\"].apply(lambda x: remove_stopwords(x))\n",
    "print(train_df[\"lowered_text_stop_removed\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5998f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [('like', 490), ('amp', 434), ('im', 419), ('fire', 357), ('get', 335), ('new', 326), ('via', 324), ('news', 282), ('people', 278), ('one', 277)]\n",
      "Most common words :  ['like', 'amp', 'im', 'fire', 'get', 'new', 'via', 'news', 'people', 'one']\n"
     ]
    }
   ],
   "source": [
    "# Removing most frequent 10 words\n",
    "from collections import Counter\n",
    "counter=Counter()\n",
    "for text in train_df[\"lowered_text_stop_removed\"]:\n",
    "    for word in text.split():\n",
    "        counter[word]+=1\n",
    "most_cmn_list=counter.most_common(10)\n",
    "print(type(most_cmn_list), most_cmn_list)\n",
    "most_cmn_words_list=[]\n",
    "for word, freq in most_cmn_list:\n",
    "    most_cmn_words_list.append(word)\n",
    "print('Most common words : ', most_cmn_words_list)\n",
    "\n",
    "def remove_frequent(in_str):\n",
    "    new_str=''\n",
    "    for word in in_str.split():\n",
    "        if word not in most_cmn_words_list:\n",
    "            new_str=new_str + word + \" \"\n",
    "    return new_str\n",
    "\n",
    "train_df[\"lowered_text_stop_removed_freq_removed\"]=train_df['lowered_text_stop_removed'].apply(lambda x: remove_frequent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cef2f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most rare words :  ['httptcotjpylu9fox', 'httptcopfavw5qyqe', 'httptcohkut5msdtp', 'issuicide', 'rajman', 'hasaka', 'risen', 'fasteners', 'xrwn', 'httptcoutbxlcbiuy']\n"
     ]
    }
   ],
   "source": [
    "# Removing 10 most rare words\n",
    "most_rare_list=counter.most_common()[-10:]\n",
    "most_rare_words=[]\n",
    "for word, freq in most_rare_list:\n",
    "    most_rare_words.append(word)\n",
    "print('Most rare words : ',most_rare_words)\n",
    "\n",
    "def remove_rare(in_text):\n",
    "    new_text=\"\"\n",
    "    for word in in_text.split():\n",
    "        if word not in most_rare_words:\n",
    "            new_text=new_text + word + \" \"\n",
    "    return new_text\n",
    "\n",
    "train_df[\"lowered_stop_freq_rare_removed\"]=train_df[\"lowered_text_stop_removed_freq_removed\"].apply(lambda x: remove_rare(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6560f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "\n",
    "def do_lemmatizing(in_str):\n",
    "    new_str=\"\"\n",
    "    for word in in_str.split():\n",
    "        new_str=new_str + lem.lemmatize(word) + \" \"\n",
    "    return new_str\n",
    "\n",
    "train_df[\"Lemmatized\"]=train_df[\"lowered_stop_freq_rare_removed\"].apply(lambda x: do_lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd467a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URLs\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "def remove_html(in_str):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', in_str)\n",
    "\n",
    "train_df[\"urls_removed\"]=train_df[\"Lemmatized\"].apply(lambda x: remove_urls(x))\n",
    "train_df[\"html_removed\"]=train_df[\"urls_removed\"].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2263902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting chat words to actual text\n",
    "chat_words_str = \"\"\"\n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher\n",
    "\"\"\"\n",
    "\n",
    "chat_words_expanded_dict = {}\n",
    "chat_words_list = []\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "    if line != \"\":\n",
    "        chat_word = line.split(\"=\")[0]\n",
    "        chat_word_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(chat_word)\n",
    "        chat_words_expanded_dict[chat_word] = chat_word_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "def convert_chat_words(in_str):\n",
    "    new_str = \"\"\n",
    "    for w in in_str.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_str = new_str + chat_words_expanded_dict[w.upper()] + \" \"\n",
    "        else:\n",
    "            new_str = new_str + w + \" \"\n",
    "    return new_str\n",
    "\n",
    "train_df[\"chat_words_coverted\"]=train_df[\"html_removed\"].apply(lambda x: convert_chat_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a6e5590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Using cached pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.6.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7f3d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"spellings_corrected\"]=train_df[\"chat_words_coverted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "300ba67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 4)\n",
      "(10876, 13)\n",
      "(7613, 5)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape)\n",
    "print(train_df.shape)\n",
    "print(train_df_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52895317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 13)\n",
      "(7613, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishalverma/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/Users/vishalverma/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/vishalverma/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>lowered_text</th>\n",
       "      <th>lowered_text_stop_removed</th>\n",
       "      <th>lowered_text_stop_removed_freq_removed</th>\n",
       "      <th>lowered_stop_freq_rare_removed</th>\n",
       "      <th>Lemmatized</th>\n",
       "      <th>urls_removed</th>\n",
       "      <th>html_removed</th>\n",
       "      <th>chat_words_coverted</th>\n",
       "      <th>spellings_corrected</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive You</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>deed reason earthquake may allah forgive You</td>\n",
       "      <td>deed reason earthquake may allah forgive You</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest near la ronge sask canada</td>\n",
       "      <td>forest near la ronge sask canada</td>\n",
       "      <td>forest near la ronge sask canada</td>\n",
       "      <td>forest near la ronge sask canada</td>\n",
       "      <td>forest near la ronge sask canada</td>\n",
       "      <td>forest near la ronge sask canada</td>\n",
       "      <td>forest near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 receive wildfire evacuation order califo...</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>13000 receive wildfires evacuation orders cali...</td>\n",
       "      <td>13000 receive wildfires evacuation orders cali...</td>\n",
       "      <td>13000 receive wildfire evacuation order califo...</td>\n",
       "      <td>13000 receive wildfire evacuation order califo...</td>\n",
       "      <td>13000 receive wildfire evacuation order califo...</td>\n",
       "      <td>13000 receive wildfire evacuation order califo...</td>\n",
       "      <td>13000 receive wildfire evacuation order califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN      deed reason earthquake may allah forgive You    \n",
       "1   4     NaN      NaN                  forest near la ronge sask canada    \n",
       "2   5     NaN      NaN  resident asked shelter place notified officer ...   \n",
       "3   6     NaN      NaN  13000 receive wildfire evacuation order califo...   \n",
       "4   7     NaN      NaN  got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                        lowered_text  \\\n",
       "0  our deeds are the reason of this earthquake ma...   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  all residents asked to shelter in place are be...   \n",
       "3  13000 people receive wildfires evacuation orde...   \n",
       "4  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                           lowered_text_stop_removed  \\\n",
       "0      deeds reason earthquake may allah forgive us    \n",
       "1             forest fire near la ronge sask canada    \n",
       "2  residents asked shelter place notified officer...   \n",
       "3  13000 people receive wildfires evacuation orde...   \n",
       "4  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "              lowered_text_stop_removed_freq_removed  \\\n",
       "0      deeds reason earthquake may allah forgive us    \n",
       "1                  forest near la ronge sask canada    \n",
       "2  residents asked shelter place notified officer...   \n",
       "3  13000 receive wildfires evacuation orders cali...   \n",
       "4  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "                      lowered_stop_freq_rare_removed  \\\n",
       "0      deeds reason earthquake may allah forgive us    \n",
       "1                  forest near la ronge sask canada    \n",
       "2  residents asked shelter place notified officer...   \n",
       "3  13000 receive wildfires evacuation orders cali...   \n",
       "4  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "                                          Lemmatized  \\\n",
       "0        deed reason earthquake may allah forgive u    \n",
       "1                  forest near la ronge sask canada    \n",
       "2  resident asked shelter place notified officer ...   \n",
       "3  13000 receive wildfire evacuation order califo...   \n",
       "4  got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                        urls_removed  \\\n",
       "0        deed reason earthquake may allah forgive u    \n",
       "1                  forest near la ronge sask canada    \n",
       "2  resident asked shelter place notified officer ...   \n",
       "3  13000 receive wildfire evacuation order califo...   \n",
       "4  got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                        html_removed  \\\n",
       "0        deed reason earthquake may allah forgive u    \n",
       "1                  forest near la ronge sask canada    \n",
       "2  resident asked shelter place notified officer ...   \n",
       "3  13000 receive wildfire evacuation order califo...   \n",
       "4  got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                 chat_words_coverted  \\\n",
       "0      deed reason earthquake may allah forgive You    \n",
       "1                  forest near la ronge sask canada    \n",
       "2  resident asked shelter place notified officer ...   \n",
       "3  13000 receive wildfire evacuation order califo...   \n",
       "4  got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                 spellings_corrected  target  \n",
       "0      deed reason earthquake may allah forgive You        1  \n",
       "1                  forest near la ronge sask canada        1  \n",
       "2  resident asked shelter place notified officer ...       1  \n",
       "3  13000 receive wildfire evacuation order califo...       1  \n",
       "4  got sent photo ruby alaska smoke wildfire pour...       1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting dataframe into train_df and test_df\n",
    "train_df_copy2 = train_df\n",
    "\n",
    "test_df = train_df.iloc[7613:,:]\n",
    "train_df = train_df.iloc[:7613,:]\n",
    "\n",
    "test_df['text'] = train_df_copy2.iloc[7613:,:]['spellings_corrected']\n",
    "train_df['text'] = train_df_copy2.iloc[:7613,:]['spellings_corrected']\n",
    "\n",
    "print(test_df.shape)\n",
    "print(train_df.shape)\n",
    "\n",
    "train_df['target'] = train_df_copy['target'].values\n",
    "# 7613\n",
    "# 3263\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93eccd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 13)\n",
      "(7613, 14)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66425f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e161228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd7b4c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4164)\t1\n",
      "  (0, 16478)\t1\n",
      "  (0, 4880)\t1\n",
      "  (0, 13831)\t1\n",
      "  (0, 1104)\t1\n",
      "  (0, 5904)\t1\n",
      "  (0, 20821)\t1\n",
      "  (1, 5895)\t1\n",
      "  (1, 14603)\t1\n",
      "  (1, 12989)\t1\n",
      "  (1, 16991)\t1\n",
      "  (1, 17261)\t1\n",
      "  (1, 2767)\t1\n",
      "  (2, 16731)\t1\n",
      "  (2, 1528)\t1\n",
      "  (2, 17623)\t2\n",
      "  (2, 15725)\t2\n",
      "  (2, 14861)\t1\n",
      "  (2, 15015)\t1\n",
      "  (2, 5293)\t1\n",
      "  (2, 15200)\t1\n",
      "  (2, 5383)\t1\n",
      "  (3, 5293)\t1\n",
      "  (3, 15200)\t1\n",
      "  (3, 138)\t1\n",
      "  :\t:\n",
      "  (7610, 6790)\t1\n",
      "  (7610, 6)\t1\n",
      "  (7610, 19922)\t1\n",
      "  (7610, 11677)\t1\n",
      "  (7611, 2823)\t1\n",
      "  (7611, 15816)\t1\n",
      "  (7611, 16850)\t1\n",
      "  (7611, 12145)\t1\n",
      "  (7611, 13331)\t1\n",
      "  (7611, 17510)\t1\n",
      "  (7611, 12262)\t1\n",
      "  (7611, 19151)\t1\n",
      "  (7611, 3398)\t1\n",
      "  (7611, 4902)\t2\n",
      "  (7611, 15867)\t1\n",
      "  (7611, 18561)\t1\n",
      "  (7611, 14803)\t1\n",
      "  (7612, 20457)\t1\n",
      "  (7612, 2726)\t1\n",
      "  (7612, 7054)\t1\n",
      "  (7612, 13070)\t1\n",
      "  (7612, 14832)\t1\n",
      "  (7612, 756)\t1\n",
      "  (7612, 16423)\t1\n",
      "  (7612, 11591)\t1\n",
      "  (0, 2823)\t1\n",
      "  (0, 3751)\t1\n",
      "  (0, 6713)\t1\n",
      "  (0, 18959)\t1\n",
      "  (1, 3212)\t1\n",
      "  (1, 4426)\t1\n",
      "  (1, 4880)\t1\n",
      "  (1, 5316)\t1\n",
      "  (1, 6834)\t1\n",
      "  (1, 17161)\t1\n",
      "  (1, 18341)\t1\n",
      "  (2, 849)\t1\n",
      "  (2, 2785)\t1\n",
      "  (2, 5786)\t1\n",
      "  (2, 5895)\t1\n",
      "  (2, 15834)\t1\n",
      "  (2, 17286)\t1\n",
      "  (2, 18221)\t1\n",
      "  (2, 18455)\t1\n",
      "  (3, 1351)\t1\n",
      "  (3, 13259)\t1\n",
      "  (3, 18208)\t1\n",
      "  (3, 20457)\t1\n",
      "  (4, 343)\t1\n",
      "  (4, 3123)\t1\n",
      "  :\t:\n",
      "  (3259, 13434)\t1\n",
      "  (3259, 15908)\t1\n",
      "  (3259, 16827)\t1\n",
      "  (3259, 18394)\t1\n",
      "  (3259, 18423)\t1\n",
      "  (3259, 20526)\t1\n",
      "  (3259, 20612)\t1\n",
      "  (3259, 20767)\t1\n",
      "  (3260, 3098)\t1\n",
      "  (3260, 4289)\t1\n",
      "  (3260, 6494)\t1\n",
      "  (3260, 13296)\t1\n",
      "  (3261, 6799)\t1\n",
      "  (3261, 7839)\t1\n",
      "  (3261, 11826)\t1\n",
      "  (3261, 12335)\t1\n",
      "  (3261, 13915)\t1\n",
      "  (3261, 15265)\t1\n",
      "  (3261, 20308)\t1\n",
      "  (3262, 859)\t1\n",
      "  (3262, 3215)\t1\n",
      "  (3262, 5051)\t1\n",
      "  (3262, 14437)\t1\n",
      "  (3262, 15731)\t1\n",
      "  (3262, 20858)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors)\n",
    "print(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e84580b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(random_state=0)\n",
    "clf_lr.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa589b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub1[\"target\"] = clf_lr.predict(test_vectors)\n",
    "sample_sub1.to_csv(\"/Users/vishalverma/Vishal/Kaggle/NLP on Tweets/results/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd3b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
